---
title: 介绍一下机器人学习
tags:
  - 机器人
  - 机器学习
  - 技术
categories:
  - 技术
  - 认知机器
top_img: false
cover: 'https://img-1253324855.cos.ap-chengdu.myqcloud.com/picgo/20210724222423.webp'
abbrlink: 12331
---

机器人学习Robot learning是一个曝光量没那么高的课题，不过其内容应用甚广，初略写一篇介绍文章，认识一下这个名字。

## 什么是机器人学习

> 机器人学习是研究机器人如何模拟人类进而实现人类的学习行为，从而能够像人类一样通过不断的学习来改善自身的性能，提高自身的适应能力和智能化水平。机器人学习是机器人学领域一个非常重要的研究方向，尤其是近几十年来一直是研究者研究的重点。

**机器人学习**是[机器学习和](https://en.wikipedia.org/wiki/Machine_learning)[机器人](https://en.wikipedia.org/wiki/Robotics)学的交叉学科。研究的技术允许机器人通过学习算法获得新的技能或适应特定的环境。机器人是一个包含一个嵌入系统的物理结构，在运用机器学习时有一些机器人独有的特点（例如高维度、收集数据和实时学习、感知系统和运动系统协同作业等）

机器人学习与[适应性控制](https://en.wikipedia.org/wiki/Adaptive_control)、[强化学习和](https://en.wikipedia.org/wiki/Reinforcement_learning)发展型机器人学密切相关，后者研究如何让机器人自主地持续地获取知识的问题。虽然机器人环境中的[计算机视觉](https://en.wikipedia.org/wiki/Computer_vision)算法经常使用[机器学习](https://en.wikipedia.org/wiki/Machine_learning)，但这些应用通常不称为"机器人学习"。

- 机器人学习是应用机器学习范式来提高现代机器人系统的能力。
- 现代机器人系统的所有部分都需要通过（机器人）学习来改进。

## 机器人学习的发展

关于这个问题的Flood sung做了很好的总结：

[最前沿：机器人学习Robot Learning的发展 - Flood Sung的文章 - 知乎](https://zhuanlan.zhihu.com/p/26988866)

> Robot Learning从目前来看，经过了以下研究思路的发展：

> （1）利用传统的控制算法结合深度学习来实现机器人端到端的控制。这个方法主要是以Guided Policy Search（GPS）为首。这个方法是Sergey Levine提出的，通过与传统方法结合，确实可以让机器人学习出一些有意思的技能，但是有个根本问题摆在面前，就是传统方法通常需要知道整个系统的模型，而这在实际的机器人中非常难以适用。就比如四轴飞行器的控制，我们可以通过外部的Vicon设备来精确的定位四轴飞行器的位置，从而实现对其精确控制，但是在户外，我们根本就做不到这点，也就无法精确建模。因此，还依赖传统方法是没有出路的，我们使用深度学习就是要抛弃传统方法的弊端。

> （2）深度增强学习DRL。由于DeepMind在DRL取得了巨大成功，而DRL就是面向决策与控制问题，特别适用于机器人，因此想在机器人上使用DRL是一种必然的想法。Google Brain团队（依然以Sergey Levine为首）做出了一些进展，在我们之前的专栏文章中也有分析[最前沿 之 谷歌的协作机械臂 - 知乎专栏](https://zhuanlan.zhihu.com/p/22758556) 。但是在使用DRL之后，DRL的弊端也就显现出来了，那就是需要大量的尝试来获取数据。对于这个问题，在机器人仿真环境还好，但是在真实的机器人上就根本没办法这么做了。为了解决这个问题，也就引出来下面两个研究思路。

> （3）迁移学习Transfer Learning。既然在真实环境不行，而仿真环境可以，那么是不是可以先在仿真环境中训练好，再把知识迁移到真实机器人上。Google Deepmind在这一块做了一些不错的工作，提出了Progressive Neural Net和PathNet，验证了迁移的可能性。而且很显然的，仿真环境越真实，迁移效果会越好。那么，搞一个非常仿真的环境就非常有意义了。这不，Nvidia 刚刚推出Isaac机器人模拟系统，确实是对Robot Learning的研究注入了一剂强心剂。

> （4）Imitation Learning 模仿学习/Few Shot Learning 少样本学习/ Meta Learning 学会学习。这是另一条思路，那就是尽量减少数据的使用量。我们如果能够教机器人几次机器人就能学会技能那么问题也能解决。而这一块也就是OpenAI (依然是Sergey Levine）那帮人在如火如荼的研究的方向。而且特别是Meta Learning，直指通用人工智能的核心。如果能够在Meta Learning上取得突破，那么本身会是革命性的。

## 机器人学习的范畴

- 通常，机器人学习涉及机器人（控制）系统的以下四个部分（SMPA架构）

    ![https://raw.githubusercontent.com/Uzizkp/image-host/master/img/picgo/20210722213501.png](https://raw.githubusercontent.com/Uzizkp/image-host/master/img/picgo/20210722213501.png)

    ### 感知系统 Sensory System

    - 机器人感知系统是将 "外部世界"（环境）的属性映射到定义明确的内部状态（实值，离散状态）。
    - 感知系统以某种方式对环境进行测量，或者对环境中的不同属性进行测量。
    - 外部世界和传感器都会受到噪音、缺失或错误数据的影响。
    - 学习适当的传感器映射是机器人学习的一个常见课题。

    ### 建模系统/识别系统  Modeling System

    - 建模系统的任务是在以下基础上产生一个周围世界的内部模型
        - 感知系统的结果。
        - 以前的建模结果。
        - 先前的知识。
    - 建模步骤通常伴随着一种模式识别方法
    - 统计学习方法是实现模式识别模块的一种现代方法

    ### 控制器，行为规划模块 Controller / Planner

    - 控制器的任务是计划并产生一套适当的行动，以应对
        - 环境变化
        - 整体任务/目标
        - 基础行为模型
        - 机器人的某种能力
    - 规划行动是机器人控制中最具挑战性的部分之一。在这一领域，自适应和学习方法还没有建立，是一片蓝海

    ### 运动控制/运动执行/行动 Motor Control / Act

    - 执行（计划）行动是由电机控制器完成的，他们必须可以应付：
        - 不精确的执行器（马达、硬件...）
        - 来自环境的不可预见的干扰。
        - 执行器系统的特性和限制。
    - 控制理论领域已经很成熟，可以管理电机指令的执行。控制执行器通常是通过闭环控制方案进行的。
    - 自适应和学习是这一部分的一个常见模块

## 机器人学习的应用

举几个例子

### 足球机器人

- 足球机器人需要学习什么？
    - 球的实时颜色
    - 对手是什么样子的
    - 其自身的身体动态
    - 走路的稳定性
    - 如何将球踢向某个方向
    - 如何接近球进行踢球
    - 避开障碍物和向球门运动之间的平衡
    - 对手的行为
    - ...
- 学习机器人姿态估计校准
    - 加速器不能区分重力和其他加速度
        - 例如机器人跌倒时，姿态很难估计
    - 陀螺仪只报告旋转速度，需要为积分设置一个起点
- 学习足球检测
    - 使用颜色提取检测候选球
    - 提取球的亮度和颜色，尺寸归一化
    - 使用神经网络
    - 用600张图片训练：160真的球，440个假的
    - 在掌上电脑上显示实时性能
- 学习运动模型
    - 学习不受干扰的行走过程中的躯干姿态模型
    - 获取不稳定措施的总偏差
    - 稳定反应（如何在不稳定状态下调整姿态）
        - 减速
        - 停止
        - 跨步
- 为什么需要陀螺仪反馈和相位重制
    - 陀螺仪反馈用来稳定躯干
    - 相位重制产生夹带效应

### 通信机器人

- 需要学习什么
    - 环境地图
    - 房间的声学特性
    - 人群检测的先验因素
    - 其交流伙伴的模样
    - 识别手势
    - 识别面部表情
    - 理解语音
    - ...